{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator - by Meike Zehlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Oct 3, 2017\n",
    "@author: meike.zehlike\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, uuid\n",
    "import itertools\n",
    "\n",
    "\n",
    "class SyntheticDatasetCreator(object):\n",
    "\n",
    "    \"\"\"\n",
    "    a dataframe that contains protected and non-protected features in columns. Each row represents\n",
    "    a candidate with their feature values\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self.__dataset\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    refers to possible combinations of protected attributes. Each group is an element of the Cartesian\n",
    "    product of the element set per protected attribute.\n",
    "    example:   attribute gender has two possible elements {0, 1}, attribute ethnicity has three\n",
    "               possible elements {0, 1, 2} --> there are six groups\n",
    "               a group is determined by one of the tuples (0, 0), (0,1), (1, 0), ..., (2, 1)\n",
    "    the non-protected group is always represented by the tuple with only zeros\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def groups(self):\n",
    "        return self.__groups\n",
    "\n",
    "\n",
    "    def __init__(self, size, attributeNamesAndCategories, nonProtectedAttributes):\n",
    "        \"\"\"\n",
    "        TODO: Parameter description\n",
    "        mu and sigma as parameters\n",
    "        \"\"\"\n",
    "        self.__dataset = pd.DataFrame()\n",
    "\n",
    "        # determine groups of candidates\n",
    "        self.__determineGroups(attributeNamesAndCategories)\n",
    "\n",
    "        # generate distribution of protected attributes\n",
    "        self.__createCategoricalProtectedAttributes(attributeNamesAndCategories, size)\n",
    "\n",
    "        # generate scores per group\n",
    "        self.__createScoresNormalDistribution(nonProtectedAttributes)\n",
    "\n",
    "        # generate ID column\n",
    "        # self.__dataset['uuid'] = uuid.uuid4()\n",
    "\n",
    "\n",
    "\n",
    "    def writeToJSON(self, path):\n",
    "        self.__dataset.to_json(path, orient='records', lines=True)\n",
    "\n",
    "\n",
    "    def __determineGroups(self, attributeNamesAndCategories):\n",
    "        elementSets = []\n",
    "        for attr, cardinality in attributeNamesAndCategories.items():\n",
    "            elementSets.append(list(range(0, cardinality)))\n",
    "\n",
    "        self.__groups = list(itertools.product(*elementSets))\n",
    "\n",
    "\n",
    "    def __createScoresNormalDistribution(self, nonProtectedAttributes):\n",
    "        \"\"\"\n",
    "        @param nonProtectedAttributes:     a string array that contains the names of the non-protected\n",
    "                                           features\n",
    "        @param mu:                         float array that contains means of the expected scores. Its\n",
    "                                           length should match the length of 'nonProtectedAttributes'\n",
    "        @param sigma:                      float array that contains standard deviations of the\n",
    "                                           expected scores. Its length should match the length of\n",
    "                                           'nonProtectedAttributes'\n",
    "        \"\"\"\n",
    "        # if len(mu_diff) != len(nonProtectedAttributes) or len(sigma_diff) != len(nonProtectedAttributes):\n",
    "        #    raise ValueError(\"lengths of arrays nonProtectedAttributes, mu_diff and sigma_diff have to match\")\n",
    "\n",
    "        def score(x, colName):\n",
    "            mu = np.random.uniform()\n",
    "            sigma = np.random.uniform()\n",
    "            x[colName] = np.random.normal(mu, sigma, size=len(x))\n",
    "            return x\n",
    "\n",
    "        for attr in nonProtectedAttributes:\n",
    "            self.__dataset = self.__dataset.groupby(self.__dataset.columns.tolist(), as_index=False,\n",
    "                                                    sort=False).apply(score, (attr))\n",
    "\n",
    "\n",
    "    def __createCategoricalProtectedAttributes(self, attributeNamesAndCategories, numItems):\n",
    "        \"\"\"\n",
    "        @param attributeNamesAndCategories:         a dictionary that contains the names of the\n",
    "                                                    protected attributes as keys and the number of\n",
    "                                                    categories as values\n",
    "                                                    (e.g. {('ethnicity'; 5), ('gender'; 2)})\n",
    "        @param numItems:                            number of items in entire created dataset (all\n",
    "                                                    protection status)\n",
    "        @return category zero is assumed to be the non-protected\n",
    "        \"\"\"\n",
    "        newData = pd.DataFrame(columns=attributeNamesAndCategories.keys())\n",
    "\n",
    "        for attributeName in attributeNamesAndCategories.keys():\n",
    "            col = []\n",
    "            categories = range(0, attributeNamesAndCategories[attributeName])\n",
    "            for count in range(0, numItems):\n",
    "                col.append(random.choice(categories))\n",
    "            newData[attributeName] = col\n",
    "\n",
    "        # add protected columns to dataset\n",
    "        self.__dataset = self.__dataset.append(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Group   Quality\n",
      "0       1  0.575929\n",
      "1       0  0.853406\n",
      "2       1  0.680415\n",
      "3       0  0.937333\n",
      "4       1  0.931634\n",
      "5       0  0.362709\n",
      "6       1  0.959965\n",
      "7       0  1.744773\n",
      "8       2  0.288077\n",
      "9       0  0.686263\n",
      "10      2  0.589445\n",
      "11      2  0.339028\n",
      "12      0  0.632483\n",
      "13      1  0.740875\n",
      "14      2  0.218936\n",
      "15      2 -0.044681\n",
      "16      2 -0.174415\n",
      "17      2 -0.173453\n",
      "18      0  1.490978\n",
      "19      0  0.975311\n",
      "20      1  0.602730\n",
      "21      1  0.702830\n",
      "22      1  0.490251\n",
      "23      2 -0.007186\n",
      "24      2  0.380672\n",
      "25      0  1.259043\n",
      "26      0  1.229382\n",
      "27      2 -0.209196\n",
      "28      0  1.424114\n",
      "29      2  0.091977\n",
      "30      0  1.050730\n",
      "31      2  0.038144\n",
      "32      0  0.771473\n",
      "33      2 -0.061866\n",
      "34      0  1.062087\n",
      "35      1  1.088182\n",
      "36      0 -0.048808\n",
      "37      1  0.633842\n",
      "38      2  0.356031\n",
      "39      2 -0.298418\n",
      "40      2  0.170707\n",
      "41      0  0.407505\n",
      "42      2 -0.285149\n",
      "43      0  0.474523\n",
      "44      0  1.461412\n",
      "45      0  0.319528\n",
      "46      2  0.206757\n",
      "47      1  0.576923\n",
      "48      2 -0.067094\n",
      "49      2 -0.057520\n"
     ]
    }
   ],
   "source": [
    "test = SyntheticDatasetCreator(50, {\"Group\":3}, [\"Quality\"])\n",
    "test.writeToJSON('test_data_set_20.json')\n",
    "print test.dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
