{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generator - by Meike Zehlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Created on Oct 3, 2017\n",
    "@author: meike.zehlike\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random, uuid\n",
    "import itertools\n",
    "\n",
    "\n",
    "class SyntheticDatasetCreator(object):\n",
    "\n",
    "    \"\"\"\n",
    "    a dataframe that contains protected and non-protected features in columns. Each row represents\n",
    "    a candidate with their feature values\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self.__dataset\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    refers to possible combinations of protected attributes. Each group is an element of the Cartesian\n",
    "    product of the element set per protected attribute.\n",
    "    example:   attribute gender has two possible elements {0, 1}, attribute ethnicity has three\n",
    "               possible elements {0, 1, 2} --> there are six groups\n",
    "               a group is determined by one of the tuples (0, 0), (0,1), (1, 0), ..., (2, 1)\n",
    "    the non-protected group is always represented by the tuple with only zeros\n",
    "    \"\"\"\n",
    "    @property\n",
    "    def groups(self):\n",
    "        return self.__groups\n",
    "\n",
    "\n",
    "    def __init__(self, size, attributeNamesAndCategories, nonProtectedAttributes):\n",
    "        \"\"\"\n",
    "        TODO: Parameter description\n",
    "        mu and sigma as parameters\n",
    "        \"\"\"\n",
    "        self.__dataset = pd.DataFrame()\n",
    "\n",
    "        # determine groups of candidates\n",
    "        self.__determineGroups(attributeNamesAndCategories)\n",
    "\n",
    "        # generate distribution of protected attributes\n",
    "        self.__createCategoricalProtectedAttributes(attributeNamesAndCategories, size)\n",
    "\n",
    "        # generate scores per group\n",
    "        self.__createScoresNormalDistribution(nonProtectedAttributes)\n",
    "\n",
    "        # generate ID column\n",
    "        # self.__dataset['uuid'] = uuid.uuid4()\n",
    "\n",
    "\n",
    "\n",
    "    def writeToJSON(self, path):\n",
    "        self.__dataset.to_json(path, orient='records', lines=True)\n",
    "\n",
    "\n",
    "    def __determineGroups(self, attributeNamesAndCategories):\n",
    "        elementSets = []\n",
    "        for attr, cardinality in attributeNamesAndCategories.items():\n",
    "            elementSets.append(list(range(0, cardinality)))\n",
    "\n",
    "        self.__groups = list(itertools.product(*elementSets))\n",
    "\n",
    "\n",
    "    def __createScoresNormalDistribution(self, nonProtectedAttributes):\n",
    "        \"\"\"\n",
    "        @param nonProtectedAttributes:     a string array that contains the names of the non-protected\n",
    "                                           features\n",
    "        @param mu:                         float array that contains means of the expected scores. Its\n",
    "                                           length should match the length of 'nonProtectedAttributes'\n",
    "        @param sigma:                      float array that contains standard deviations of the\n",
    "                                           expected scores. Its length should match the length of\n",
    "                                           'nonProtectedAttributes'\n",
    "        \"\"\"\n",
    "        # if len(mu_diff) != len(nonProtectedAttributes) or len(sigma_diff) != len(nonProtectedAttributes):\n",
    "        #    raise ValueError(\"lengths of arrays nonProtectedAttributes, mu_diff and sigma_diff have to match\")\n",
    "\n",
    "        def score(x, colName):\n",
    "            mu = np.random.uniform()\n",
    "            sigma = np.random.uniform()\n",
    "            x[colName] = np.random.normal(mu, sigma, size=len(x))\n",
    "            return x\n",
    "\n",
    "        for attr in nonProtectedAttributes:\n",
    "            self.__dataset = self.__dataset.groupby(self.__dataset.columns.tolist(), as_index=False,\n",
    "                                                    sort=False).apply(score, (attr))\n",
    "\n",
    "\n",
    "    def __createCategoricalProtectedAttributes(self, attributeNamesAndCategories, numItems):\n",
    "        \"\"\"\n",
    "        @param attributeNamesAndCategories:         a dictionary that contains the names of the\n",
    "                                                    protected attributes as keys and the number of\n",
    "                                                    categories as values\n",
    "                                                    (e.g. {('ethnicity'; 5), ('gender'; 2)})\n",
    "        @param numItems:                            number of items in entire created dataset (all\n",
    "                                                    protection status)\n",
    "        @return category zero is assumed to be the non-protected\n",
    "        \"\"\"\n",
    "        newData = pd.DataFrame(columns=attributeNamesAndCategories.keys())\n",
    "\n",
    "        for attributeName in attributeNamesAndCategories.keys():\n",
    "            col = []\n",
    "            categories = range(0, attributeNamesAndCategories[attributeName])\n",
    "            for count in range(0, numItems):\n",
    "                col.append(random.choice(categories))\n",
    "            newData[attributeName] = col\n",
    "\n",
    "        # add protected columns to dataset\n",
    "        self.__dataset = self.__dataset.append(newData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Group   Quality\n",
      "0        3 -0.039840\n",
      "1        0  0.731639\n",
      "2        3  0.373630\n",
      "3        3  0.328663\n",
      "4        2  0.206999\n",
      "5        3  0.824956\n",
      "6        0  0.500127\n",
      "7        0  1.742901\n",
      "8        0  0.675217\n",
      "9        3  0.373685\n",
      "10       1  0.773616\n",
      "11       1  0.388889\n",
      "12       0  0.384533\n",
      "13       1  0.388438\n",
      "14       0  0.920718\n",
      "15       3  0.521755\n",
      "16       2  0.429529\n",
      "17       1  0.616314\n",
      "18       1  0.630154\n",
      "19       2  0.099669\n",
      "20       2  0.004991\n",
      "21       0  1.047730\n",
      "22       3  0.353288\n",
      "23       3  0.355112\n",
      "24       2  0.051228\n",
      "25       3  0.561300\n",
      "26       2  0.151556\n",
      "27       1  0.523969\n",
      "28       1  0.700359\n",
      "29       3  0.248716\n",
      "..     ...       ...\n",
      "470      1  1.057143\n",
      "471      3  0.709258\n",
      "472      3  0.618109\n",
      "473      2  0.413723\n",
      "474      0  1.851140\n",
      "475      1  0.685880\n",
      "476      1  0.845164\n",
      "477      3  0.187097\n",
      "478      2  0.269223\n",
      "479      2  0.320761\n",
      "480      3  0.221130\n",
      "481      2  0.011889\n",
      "482      3  0.192882\n",
      "483      1  0.986052\n",
      "484      1  0.630295\n",
      "485      0  2.293437\n",
      "486      3  0.263564\n",
      "487      2  0.129958\n",
      "488      3  0.409498\n",
      "489      0  1.086182\n",
      "490      3  0.501585\n",
      "491      3  0.428683\n",
      "492      1  0.704099\n",
      "493      1  0.857009\n",
      "494      1  0.856830\n",
      "495      3  0.362248\n",
      "496      2  0.463840\n",
      "497      3  0.389707\n",
      "498      3  0.318859\n",
      "499      2  0.364801\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "test = SyntheticDatasetCreator(500, {\"Group\":4}, [\"Quality\"])\n",
    "test.writeToJSON('test_data_set_sample_500.json')\n",
    "print test.dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
